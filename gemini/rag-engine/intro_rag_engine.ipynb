{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishek-beings/rag-engine-gcp-collab/blob/main/gemini/rag-engine/intro_rag_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Intro to Building a Scalable and Modular RAG System with RAG Engine in Vertex AI\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Frag-engine%2Fintro_rag_engine.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/rag-engine/intro_rag_engine.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Holt Skinner](https://github.com/holtskinner) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Retrieval Augmented Generation (RAG) improves Large Language Models (LLMs) by allowing them to access and process external information sources during generation. This ensures the model's responses are grounded in factual data and avoids hallucinations.\n",
        "\n",
        "A common problem with LLMs is that they don't understand private knowledge, that\n",
        "is, your organization's data. With RAG Engine, you can enrich the\n",
        "LLM context with additional private information, because the model can reduce\n",
        "hallucinations and answer questions more accurately.\n",
        "\n",
        "By combining additional knowledge sources with the existing knowledge that LLMs\n",
        "have, a better context is provided. The improved context along with the query\n",
        "enhances the quality of the LLM's response.\n",
        "\n",
        "The following concepts are key to understanding Vertex AI RAG Engine. These concepts are listed in the order of the\n",
        "retrieval-augmented generation (RAG) process.\n",
        "\n",
        "1. **Data ingestion**: Intake data from different data sources. For example,\n",
        "  local files, Google Cloud Storage, and Google Drive.\n",
        "\n",
        "1. **Data transformation**: Conversion of the data in preparation for indexing. For example, data is split into chunks.\n",
        "\n",
        "1. **Embedding**: Numerical representations of words or pieces of text. These numbers capture the\n",
        "   semantic meaning and context of the text. Similar or related words or text\n",
        "   tend to have similar embeddings, which means they are closer together in the\n",
        "   high-dimensional vector space.\n",
        "\n",
        "1. **Data indexing**: RAG Engine creates an index called a corpus.\n",
        "   The index structures the knowledge base so it's optimized for searching. For\n",
        "   example, the index is like a detailed table of contents for a massive\n",
        "   reference book.\n",
        "\n",
        "1. **Retrieval**: When a user asks a question or provides a prompt, the retrieval\n",
        "  component in RAG Engine searches through its knowledge\n",
        "  base to find information that is relevant to the query.\n",
        "\n",
        "1. **Generation**: The retrieved information becomes the context added to the\n",
        "  original user query as a guide for the generative AI model to generate\n",
        "  factually grounded and relevant responses.\n",
        "\n",
        "For more information, refer to the public documentation for [Vertex AI RAG Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/rag-overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK and Google Gen AI SDK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tFy3H3aPgx12",
        "outputId": "eca1d4ca-a11e-4a11-c0fc-a081047e5b04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-cloud-aiplatform google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XRvKdaPDTznN",
        "outputId": "fa200823-6cdc-4a14-db70-4572718a37fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import os\n",
        "\n",
        "from google import genai\n",
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"aida-22a9a\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5303c05f7aa6"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6fc324893334"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "from google.genai.types import GenerateContentConfig, Retrieval, Tool, VertexRagStore\n",
        "from vertexai import rag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e43229f3ad4f"
      },
      "source": [
        "### Create a RAG Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cf93d5f0ce00"
      },
      "outputs": [],
      "source": [
        "# Currently supports Google first-party embedding models\n",
        "EMBEDDING_MODEL = \"publishers/google/models/text-embedding-005\"  # @param {type:\"string\", isTemplate: true}\n",
        "\n",
        "rag_corpus = rag.create_corpus(\n",
        "    display_name=\"my-rag-corpus-gcs\",\n",
        "    backend_config=rag.RagVectorDbConfig(\n",
        "        rag_embedding_model_config=rag.RagEmbeddingModelConfig(\n",
        "            vertex_prediction_endpoint=rag.VertexPredictionEndpoint(\n",
        "                publisher_model=EMBEDDING_MODEL\n",
        "            )\n",
        "        )\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "197c585b61b2"
      },
      "source": [
        "### Check the corpus just created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f229b13dc617",
        "outputId": "34d2b61b-d3b6-44d2-a3ee-a98618d2e703",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ListRagCorporaPager<>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "rag.list_corpora()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52924cc1440"
      },
      "source": [
        "### Upload a local file to the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4976ffe8564f"
      },
      "outputs": [],
      "source": [
        "%%writefile test.md\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by allowing them to access and incorporate external data sources when generating responses. Here's a breakdown:\n",
        "\n",
        "**What it is:**\n",
        "\n",
        "* **Combining Retrieval and Generation:**\n",
        "    * RAG combines the strengths of information retrieval systems (like search engines) with the generative power of LLMs.\n",
        "    * It enables LLMs to go beyond their pre-trained data and access up-to-date and specific information.\n",
        "* **How it works:**\n",
        "    * When a user asks a question, the RAG system first retrieves relevant information from external data sources (e.g., databases, documents, web pages).\n",
        "    * This retrieved information is then provided to the LLM as additional context.\n",
        "    * The LLM uses this augmented context to generate a more accurate and informative response.\n",
        "\n",
        "**Why it's helpful:**\n",
        "\n",
        "* **Access to Up-to-Date Information:**\n",
        "    * LLMs are trained on static datasets, so their knowledge can become outdated. RAG allows them to access real-time or frequently updated information.\n",
        "* **Improved Accuracy and Factual Grounding:**\n",
        "    * RAG reduces the risk of LLM \"hallucinations\" (generating false or misleading information) by grounding responses in verified external data.\n",
        "* **Enhanced Contextual Relevance:**\n",
        "    * By providing relevant context, RAG enables LLMs to generate more precise and tailored responses to specific queries.\n",
        "* **Increased Trust and Transparency:**\n",
        "    * RAG can provide source citations, allowing users to verify the information and increasing trust in the LLM's responses.\n",
        "* **Cost Efficiency:**\n",
        "    * Rather than constantly retraining large language models, RAG allows for the introduction of new data in a more cost effective way.\n",
        "\n",
        "In essence, RAG bridges the gap between the vast knowledge of LLMs and the need for accurate, current, and contextually relevant information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "529390917c29"
      },
      "outputs": [],
      "source": [
        "rag_file = rag.upload_file(\n",
        "    corpus_name=rag_corpus.name,\n",
        "    path=\"test.md\",\n",
        "    display_name=\"test.md\",\n",
        "    description=\"my test file\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5269a0c2786d"
      },
      "source": [
        "### Import files from Google Cloud Storage\n",
        "\n",
        "Remember to grant \"Viewer\" access to the \"Vertex RAG Data Service Agent\" (with the format of `service-{project_number}@gcp-sa-vertex-rag.iam.gserviceaccount.com`) for your Google Cloud Storage bucket.\n",
        "\n",
        "For this example, we'll use a public GCS bucket containing earning reports from Alphabet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5910ae450f69"
      },
      "outputs": [],
      "source": [
        "INPUT_GCS_BUCKET = (\n",
        "    \"gs://grouding-test\"\n",
        ")\n",
        "\n",
        "response = rag.import_files(\n",
        "    corpus_name=rag_corpus.name,\n",
        "    paths=[INPUT_GCS_BUCKET],\n",
        "    # Optional\n",
        "    transformation_config=rag.TransformationConfig(\n",
        "        chunking_config=rag.ChunkingConfig(chunk_size=1024, chunk_overlap=100)\n",
        "    ),\n",
        "    max_embedding_requests_per_min=900,  # Optional\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60a84095746d"
      },
      "source": [
        "### Import files from Google Drive\n",
        "\n",
        "Eligible paths can be formatted as:\n",
        "\n",
        "- `https://drive.google.com/drive/folders/{folder_id}`\n",
        "- `https://drive.google.com/file/d/{file_id}`.\n",
        "\n",
        "Remember to grant \"Viewer\" access to the \"Vertex RAG Data Service Agent\" (with the format of `service-{project_number}@gcp-sa-vertex-rag.iam.gserviceaccount.com`) for your Drive folder/files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a90c125874c"
      },
      "outputs": [],
      "source": [
        "response = rag.import_files(\n",
        "    corpus_name=rag_corpus.name,\n",
        "    paths=[\"https://drive.google.com/drive/folders/{folder_id}\"],\n",
        "    # Optional\n",
        "    transformation_config=rag.TransformationConfig(\n",
        "        chunking_config=rag.ChunkingConfig(chunk_size=512, chunk_overlap=50)\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f700b3e23121"
      },
      "source": [
        "### Optional: Perform direct context retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4669c5cdbb5a",
        "outputId": "3e291e91-6314-4e8f-d788-02a0fc836d34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "contexts {\n",
            "  contexts {\n",
            "    source_uri: \"gs://grouding-test/video1036176414 - Yetunde Olayiwola.mp4 (1).docx\"\n",
            "    text: \"That\\'s, that\\'s crazy. But dashboard is work. I think this still needs a lot of working now because I can\\'t see, I can\\'t seem to be able to get the text. Same recording started successful. \\n Speaker 0 (22:06 - 24:14): \\nLemme try it again and join meeting. Oh, okay. Is not share that. Okay. So anybody\\'s joined. I\\'m still waiting to see if either box is going to be joining. So I honestly don\\'t know what\\'s going on. The, um, the whole screen as the whole stuff has mumbled up. My, um, zoom meeting. I don\\'t know which is working again. Is it the, is this my normal meeting or is this, is this the IDA Bots meeting? I think it\\'s here is the, for the IDA Bots. I don\\'t know. Okay. So if that is, it\\'s, so when is, um, either bots going to join us. Hi IBOs. \\n Speaker 0 (24:36 - 26:10): \\nThere\\'s no I bots. I, anyway. Could it be the I bots? Is I, I have no idea. I think I\\'m honestly don\\'t know what\\'s going on. One sources, one sources. Um, let\\'s see the recording. Lemme try one more time and see if I would, um, be lucky this time. Now I honestly don\\'t know what\\'s going on to be honest. I do not know what\\'s going on. So I think, um, this platform still needs, um, still needs to be worked on and a lot needs to be corrected and and stopped. And this is where I had the meeting, I pray. I hope it\\'s recording.\"\n",
            "    source_display_name: \"video1036176414 - Yetunde Olayiwola.mp4 (1).docx\"\n",
            "    score: 0.44410728243668185\n",
            "    chunk {\n",
            "      text: \"That\\'s, that\\'s crazy. But dashboard is work. I think this still needs a lot of working now because I can\\'t see, I can\\'t seem to be able to get the text. Same recording started successful. \\n Speaker 0 (22:06 - 24:14): \\nLemme try it again and join meeting. Oh, okay. Is not share that. Okay. So anybody\\'s joined. I\\'m still waiting to see if either box is going to be joining. So I honestly don\\'t know what\\'s going on. The, um, the whole screen as the whole stuff has mumbled up. My, um, zoom meeting. I don\\'t know which is working again. Is it the, is this my normal meeting or is this, is this the IDA Bots meeting? I think it\\'s here is the, for the IDA Bots. I don\\'t know. Okay. So if that is, it\\'s, so when is, um, either bots going to join us. Hi IBOs. \\n Speaker 0 (24:36 - 26:10): \\nThere\\'s no I bots. I, anyway. Could it be the I bots? Is I, I have no idea. I think I\\'m honestly don\\'t know what\\'s going on. One sources, one sources. Um, let\\'s see the recording. Lemme try one more time and see if I would, um, be lucky this time. Now I honestly don\\'t know what\\'s going on to be honest. I do not know what\\'s going on. So I think, um, this platform still needs, um, still needs to be worked on and a lot needs to be corrected and and stopped. And this is where I had the meeting, I pray. I hope it\\'s recording.\"\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    source_uri: \"gs://grouding-test/video1036176414 - Yetunde Olayiwola.mp4 (1).docx\"\n",
            "    text: \"Speaker 0 (00:35 - 01:19): \\nOkay, so, um, I\\'m gonna navigate to the platform base.com and I\\'ll click get started. So, um, it\\'s opening set all, um, of this and yeah, I\\'m going to be click on get started and I\\'ll register on the platform. So I\\'m gonna register with my, um, email address \\n Speaker 0 (01:29 - 04:49): \\nAnd I\\'m gonna be, um, putting in, um, my password as well. And, um, oh, I have to get started not signing, \\'cause I don\\'t, I do not have an account there. I don\\'t wanna say. So I\\'m gonna input my first name and my last name, then my email address as requested. Yeah. Then set the password for myself. Yeah. Then I create an account. No, I don\\'t wanna save it. So I\\'m gonna just click on my email to, let me see. I\\'ll open my email. A um, a link has been sent to verify my Okay. I verify my email, email address, my account through my email. So, uh, my email has been verified so I can\\'t then return to my accounts. So I have to sign in with, um, the email and password I just created since my account has been verified. Okay. I just sign in. No, I don\\'t wanna save it. So, um, I\\'m asked to upload a video file an audio file. A text file or a PDF. Okay. Post and look. Okay. I\\'m just gonna click here and browse through my, um, looking for, um, a document to, to upload. I\\'m trying to, um, let me look for something. I can just upload and Yeah. Something that, that\\'s not, that is not so confidential. Okay. I\\'m just gonna, \\n Speaker 0 (05:04 - 09:12): \\nSo I\\'m just, um, just gonna upload. Okay. Yeah. Oh, it\\'s supposed to be a PDF. Okay. And just save it as a PDF just as long. Save it as a PDF. Yeah. Save as a PDF, save it and um, yeah, go back to where, okay. Okay. Yeah, upload. Okay. It\\'s been uploaded but a meeting using the patient integrations. Okay. So, um, I\\'ve been asked to record or using the, um, link of the meeting. Okay. Let me just copy the link of the meeting from here. I\\'m just gonna come here and paste. I\\'m just gonna record, start recording. Okay. Um, um, record. I don\\'t know if, okay, it has started record. We request to join your meeting. Please admit and let them. Okay, so I have to admit the either bot, either bots, so I\\'m looking for when ADA bot is gonna join the meeting. ADA bot, where are you waiting for you to join the meeting? So once the ADA bot joins the meeting, I\\'m gonna interact with the ADA chat feature to summarize and synthesize a single source. We are waiting for the either box. Either box is not coming up or joining the meeting. I don\\'t know. Um, let\\'s try to start the recording again. So I\\'m just gonna piece, uh, the link of the meeting and, um, sorry, let me recopy, um, the zoom link? Yeah, this is the zoom link. Um, \\n Speaker 0 (09:29 - 13:47): \\nOkay. Um, the, um, the Zoom link. Let me just, we were just gonna, um, just trying to copy the link of the meeting so that, um, yeah, this is the meeting. This is the meeting. Yeah. So, um, I don\\'t know, um, my Zoom seems to be giving me some issues. Okay. So, um, I\\'m trying to navigate and figure it out. So just, um, I\\'m just gonna paste link of the meeting here. Yeah, that\\'s too. So we are asking, joined the meeting. Oh, okay. So I\\'m waiting for the IDA bots. I can\\'t seem to find the IDA bots anywhere here, but this kind of, it\\'s kind of not easy to to navigate honestly. Look at and play.\"\n",
            "    source_display_name: \"video1036176414 - Yetunde Olayiwola.mp4 (1).docx\"\n",
            "    score: 0.46203628299407873\n",
            "    chunk {\n",
            "      text: \"Speaker 0 (00:35 - 01:19): \\nOkay, so, um, I\\'m gonna navigate to the platform base.com and I\\'ll click get started. So, um, it\\'s opening set all, um, of this and yeah, I\\'m going to be click on get started and I\\'ll register on the platform. So I\\'m gonna register with my, um, email address \\n Speaker 0 (01:29 - 04:49): \\nAnd I\\'m gonna be, um, putting in, um, my password as well. And, um, oh, I have to get started not signing, \\'cause I don\\'t, I do not have an account there. I don\\'t wanna say. So I\\'m gonna input my first name and my last name, then my email address as requested. Yeah. Then set the password for myself. Yeah. Then I create an account. No, I don\\'t wanna save it. So I\\'m gonna just click on my email to, let me see. I\\'ll open my email. A um, a link has been sent to verify my Okay. I verify my email, email address, my account through my email. So, uh, my email has been verified so I can\\'t then return to my accounts. So I have to sign in with, um, the email and password I just created since my account has been verified. Okay. I just sign in. No, I don\\'t wanna save it. So, um, I\\'m asked to upload a video file an audio file. A text file or a PDF. Okay. Post and look. Okay. I\\'m just gonna click here and browse through my, um, looking for, um, a document to, to upload. I\\'m trying to, um, let me look for something. I can just upload and Yeah. Something that, that\\'s not, that is not so confidential. Okay. I\\'m just gonna, \\n Speaker 0 (05:04 - 09:12): \\nSo I\\'m just, um, just gonna upload. Okay. Yeah. Oh, it\\'s supposed to be a PDF. Okay. And just save it as a PDF just as long. Save it as a PDF. Yeah. Save as a PDF, save it and um, yeah, go back to where, okay. Okay. Yeah, upload. Okay. It\\'s been uploaded but a meeting using the patient integrations. Okay. So, um, I\\'ve been asked to record or using the, um, link of the meeting. Okay. Let me just copy the link of the meeting from here. I\\'m just gonna come here and paste. I\\'m just gonna record, start recording. Okay. Um, um, record. I don\\'t know if, okay, it has started record. We request to join your meeting. Please admit and let them. Okay, so I have to admit the either bot, either bots, so I\\'m looking for when ADA bot is gonna join the meeting. ADA bot, where are you waiting for you to join the meeting? So once the ADA bot joins the meeting, I\\'m gonna interact with the ADA chat feature to summarize and synthesize a single source. We are waiting for the either box. Either box is not coming up or joining the meeting. I don\\'t know. Um, let\\'s try to start the recording again. So I\\'m just gonna piece, uh, the link of the meeting and, um, sorry, let me recopy, um, the zoom link? Yeah, this is the zoom link. Um, \\n Speaker 0 (09:29 - 13:47): \\nOkay. Um, the, um, the Zoom link. Let me just, we were just gonna, um, just trying to copy the link of the meeting so that, um, yeah, this is the meeting. This is the meeting. Yeah. So, um, I don\\'t know, um, my Zoom seems to be giving me some issues. Okay. So, um, I\\'m trying to navigate and figure it out. So just, um, I\\'m just gonna paste link of the meeting here. Yeah, that\\'s too. So we are asking, joined the meeting. Oh, okay. So I\\'m waiting for the IDA bots. I can\\'t seem to find the IDA bots anywhere here, but this kind of, it\\'s kind of not easy to to navigate honestly. Look at and play.\"\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    source_uri: \"gs://grouding-test/video1036176414 - Yetunde Olayiwola.mp4 (1).docx\"\n",
            "    text: \"So just, um, I\\'m just gonna paste link of the meeting here. Yeah, that\\'s too. So we are asking, joined the meeting. Oh, okay. So I\\'m waiting for the IDA bots. I can\\'t seem to find the IDA bots anywhere here, but this kind of, it\\'s kind of not easy to to navigate honestly. Look at and play. Oh, I\\'m back to the meeting. This is my screen, so I\\'m just gonna, yeah, this is the either bot, but I can\\'t, seems to find the, um, recording. I can\\'t, seems to record a meeting. Okay. Record the meeting using the locate and play recorded meeting. I don\\'t, I honestly don\\'t know where the recorded meeting is and, um, knowing Okay. Recordings. Yeah. Record meeting. Let me paste the record. Okay. Joining call session is being recorded is, \\n Speaker 0 (14:08 - 15:09): \\nI can\\'t seem to find the iPods. I don\\'t know where that is. I thought it was gonna pop up once I um, okay. Recording as such, no sources found, no resources found. Okay. No resources has been found. Okay. Um, I, and play recorded meeting. I can\\'t seem to find that. Interact with the, I I\\'m trying to record, I don\\'t know. Let me try to paste the link one more time. These are there. \\n Speaker 0 (15:25 - 17:23): \\nOkay, so no resources found, I don\\'t know why. Okay. And download the transcription of the notes. Adjust application settings. Okay. Um, let me adjust the application setting. Dark mode, light mode. The contrast, no con, no contrast. Um, okay. Navigate the layouts. Okay. If we want it like this, we want it in the middle, we want it by the side, we want it the way. Oh, okay. That makes sense. We want it compact. We don\\'t want it compact. Left. Oh, okay. This moves to the left or to the right. Sorry. And this comes to the left. So that\\'s what that does. The different mode of presentation can present like this, present like this resets, resets, family popping the size. It can reduce the size, it can increase it, you can have a more big without picture size. I think that\\'s what that is for. Then the font size, we can have different font size. Then you can have a full screen you can press escape to. I can close this and join this. So, um, I don\\'t know my, \\n Speaker 1 (17:32 - 17:32): \\nI, \\n Speaker 0 (18:14 - 21:24): \\nSo it\\'s just me. I\\'m the host. There\\'s no either chats box account, seems to find the either chat box. So let me try to reach the instruction again. Maybe I could figure it out. But honestly figuring this out is not is the instructions. The instructions aren\\'t straightforward at all. Record the meeting using the integration. Okay, let me go. Integration. This is the record button. Yes, I guess. Okay. Yeah. Meeting you are, yeah, that\\'s the meeting code. Is it? I\\'ve got that. So I just, um, I don\\'t know. So I\\'m just trying to figure out where, um, recordings, um, I guess, um, the, um, either bot is hidden somewhere. I don\\'t know what does this or English gonna be assessed. \\n Speaker 1 (21:25 - 21:25): \\nSo \\n Speaker 0 (21:28 - 22:00): \\nI have to go back, have to go back, go back to return to dashboard. I can\\'t seem to return to dashboard. That\\'s, that\\'s crazy. But dashboard is work. I think this still needs a lot of working now because I can\\'t see, I can\\'t seem to be able to get the text. Same recording started successful. \\n Speaker 0 (22:06 - 24:14): \\nLemme try it again and join meeting. Oh, okay. Is not share that. Okay. So anybody\\'s joined.\"\n",
            "    source_display_name: \"video1036176414 - Yetunde Olayiwola.mp4 (1).docx\"\n",
            "    score: 0.4661716772083152\n",
            "    chunk {\n",
            "      text: \"So just, um, I\\'m just gonna paste link of the meeting here. Yeah, that\\'s too. So we are asking, joined the meeting. Oh, okay. So I\\'m waiting for the IDA bots. I can\\'t seem to find the IDA bots anywhere here, but this kind of, it\\'s kind of not easy to to navigate honestly. Look at and play. Oh, I\\'m back to the meeting. This is my screen, so I\\'m just gonna, yeah, this is the either bot, but I can\\'t, seems to find the, um, recording. I can\\'t, seems to record a meeting. Okay. Record the meeting using the locate and play recorded meeting. I don\\'t, I honestly don\\'t know where the recorded meeting is and, um, knowing Okay. Recordings. Yeah. Record meeting. Let me paste the record. Okay. Joining call session is being recorded is, \\n Speaker 0 (14:08 - 15:09): \\nI can\\'t seem to find the iPods. I don\\'t know where that is. I thought it was gonna pop up once I um, okay. Recording as such, no sources found, no resources found. Okay. No resources has been found. Okay. Um, I, and play recorded meeting. I can\\'t seem to find that. Interact with the, I I\\'m trying to record, I don\\'t know. Let me try to paste the link one more time. These are there. \\n Speaker 0 (15:25 - 17:23): \\nOkay, so no resources found, I don\\'t know why. Okay. And download the transcription of the notes. Adjust application settings. Okay. Um, let me adjust the application setting. Dark mode, light mode. The contrast, no con, no contrast. Um, okay. Navigate the layouts. Okay. If we want it like this, we want it in the middle, we want it by the side, we want it the way. Oh, okay. That makes sense. We want it compact. We don\\'t want it compact. Left. Oh, okay. This moves to the left or to the right. Sorry. And this comes to the left. So that\\'s what that does. The different mode of presentation can present like this, present like this resets, resets, family popping the size. It can reduce the size, it can increase it, you can have a more big without picture size. I think that\\'s what that is for. Then the font size, we can have different font size. Then you can have a full screen you can press escape to. I can close this and join this. So, um, I don\\'t know my, \\n Speaker 1 (17:32 - 17:32): \\nI, \\n Speaker 0 (18:14 - 21:24): \\nSo it\\'s just me. I\\'m the host. There\\'s no either chats box account, seems to find the either chat box. So let me try to reach the instruction again. Maybe I could figure it out. But honestly figuring this out is not is the instructions. The instructions aren\\'t straightforward at all. Record the meeting using the integration. Okay, let me go. Integration. This is the record button. Yes, I guess. Okay. Yeah. Meeting you are, yeah, that\\'s the meeting code. Is it? I\\'ve got that. So I just, um, I don\\'t know. So I\\'m just trying to figure out where, um, recordings, um, I guess, um, the, um, either bot is hidden somewhere. I don\\'t know what does this or English gonna be assessed. \\n Speaker 1 (21:25 - 21:25): \\nSo \\n Speaker 0 (21:28 - 22:00): \\nI have to go back, have to go back, go back to return to dashboard. I can\\'t seem to return to dashboard. That\\'s, that\\'s crazy. But dashboard is work. I think this still needs a lot of working now because I can\\'t see, I can\\'t seem to be able to get the text. Same recording started successful. \\n Speaker 0 (22:06 - 24:14): \\nLemme try it again and join meeting. Oh, okay. Is not share that. Okay. So anybody\\'s joined.\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Direct context retrieval\n",
        "response = rag.retrieval_query(\n",
        "    rag_resources=[\n",
        "        rag.RagResource(\n",
        "            rag_corpus=rag_corpus.name,\n",
        "            # Optional: supply IDs from `rag.list_files()`.\n",
        "            # rag_file_ids=[\"rag-file-1\", \"rag-file-2\", ...],\n",
        "        )\n",
        "    ],\n",
        "    rag_retrieval_config=rag.RagRetrievalConfig(\n",
        "        top_k=10,  # Optional\n",
        "        filter=rag.Filter(\n",
        "            vector_distance_threshold=0.5,  # Optional\n",
        "        ),\n",
        "    ),\n",
        "    text=\"what is the speaker talking about?\",\n",
        ")\n",
        "print(response)\n",
        "\n",
        "# Optional: The retrieved context can be passed to any SDK or model generation API to generate final results.\n",
        "# context = \" \".join([context.text for context in response.contexts.contexts]).replace(\"\\n\", \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79ea89661842"
      },
      "source": [
        "### Create RAG Retrieval Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0ebceac3d816"
      },
      "outputs": [],
      "source": [
        "# Create a tool for the RAG Corpus\n",
        "rag_retrieval_tool = Tool(\n",
        "    retrieval=Retrieval(\n",
        "        vertex_rag_store=VertexRagStore(\n",
        "            rag_corpora=[rag_corpus.name],\n",
        "            similarity_top_k=10,\n",
        "            vector_distance_threshold=0.5,\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d88fa7ede853"
      },
      "source": [
        "### Generate Content with Gemini using RAG Retrieval Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8dd928baecd4"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.0-flash-001\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "124b36be8d5b",
        "outputId": "3f5ab321-1691-40f0-80ce-26ca236fedf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The speaker navigates to platform base.com, clicks \"get started\", and registers with their email address, first name, and last name and sets a password. After verifying the account through a link sent to their email, they sign in. The speaker uploads a PDF document. Then copies and pastes a Zoom meeting link, starts recording and waits for the ADA bot to join the meeting to interact with the ADA chat feature to summarize and synthesize a single source. The speaker seems to be experiencing some technical difficulties with Zoom."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Summarize\",\n",
        "    config=GenerateContentConfig(tools=[rag_retrieval_tool]),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0268fe43d41c"
      },
      "source": [
        "### Generate Content with Llama3 using RAG Retrieval Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6e67ee7968c"
      },
      "outputs": [],
      "source": [
        "from vertexai import generative_models\n",
        "\n",
        "# Load tool into Llama model\n",
        "rag_retrieval_tool = generative_models.Tool.from_retrieval(\n",
        "    retrieval=rag.Retrieval(\n",
        "        source=rag.VertexRagStore(\n",
        "            rag_resources=[rag.RagResource(rag_corpus=rag_corpus.name)],\n",
        "            rag_retrieval_config=rag.RagRetrievalConfig(\n",
        "                top_k=10,  # Optional\n",
        "                filter=rag.Filter(\n",
        "                    vector_distance_threshold=0.5,  # Optional\n",
        "                ),\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        ")\n",
        "\n",
        "llama_model = generative_models.GenerativeModel(\n",
        "    # your self-deployed endpoint for Llama3\n",
        "    \"projects/{project}/locations/{location}/endpoints/{endpoint_resource_id}\",\n",
        "    tools=[rag_retrieval_tool],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6d710b6dece"
      },
      "outputs": [],
      "source": [
        "response = llama_model.generate_content(\"What is RAG?\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_rag_engine.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}